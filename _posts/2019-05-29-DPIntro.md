---
title:  "Differential Privacy Part-I: Introduction"
layout: posts
mathjax: "true"
---

<p style="text-align:justify">Differential Privacy (DP) is one take on privacy which ensures that no single individual is affected for being a part of the dataset or data collection process.

$$x+y=z$$
$$/epsilon$$

DP is based on Fundamental Law of Information Recovery which states that "overly accurate answers to too many questions will destroy privacy in a spectacular way".

<br />
<br />
<img src="https://camo.githubusercontent.com/4385679354702242ee6e3add8589a716afbaa80b/68747470733a2f2f77696b696d656469612e6f72672f6170692f726573745f76312f6d656469612f6d6174682f72656e6465722f7376672f61333136306464373736633361313834313136373437623333666266643736333935356337656131">
<br />
<br />

The above equation is the foundation of Differential privacy.
Consider two databases one database without a given row of data given by D1 and the other with the given data given by D2. How likely would the output of a certain analysis (or mechanism) differ if the datapoint is included is quantified by a epsilon. This implies output of a certain mechanism with the person's data in the database is likely to be the same or lesser by an factor of 1/exp(epsilon).</p>

We charecterize this by DP(<img height="20" width="20" src="https://camo.githubusercontent.com/fbcc26741027732b93efb1ba96c51dd79b6dc404/68747470733a2f2f63646e322e69636f6e66696e6465722e636f6d2f646174612f69636f6e732f677265656b2d6c6174696e2d73796d626f6c732f32342f657073696c6f6e2d3132382e706e67">,0)

<img src="https://camo.githubusercontent.com/ea990895b47703d5d7292bab7285ddd37f2f7497/687474703a2f2f636c6576657268616e732e696f2f6173736574732f646966666572656e7469616c2d707269766163792e706e67">

<br />
<p style="text-align:justify">The above charecterization is (epislon,delta) ,
<br />
The delta term gives the tail bound of probability distribution.
To make it more intuitive as to why we quantify privacy this way,let us take an small example. Say a small town of 100 people has a total income of 200 million. If the next time you calculate this number and it ends up reducing to 180 million because there are 99 people. Using auxillary data sources you could find out who left the town from which isn't hard to infer that their income was 20 million. This could be discouraging for participants to participate in such statistics as it are leaks such sensitive information.

<br />
<br />
Differential Privacy also provides guarantee against differencing attacks. Similar to the example of calculating sum of incomes , queries could be made so as to retrieve values of whole population and without a given attribute X , the difference giving the resultant answer.

In simple words we could say that a user's privacy is violated if more information is learnt about a given user when the analysis is performed. If a general inference is made about the population and the same applies to the specific user the users privacy is not violated.

<br />

Data collection and analysis must be limited to representing statistics of entire population but in a way that individuals confidentiality is maintained.

You can get complete privacy by absolutely outputting junk or get complete utility for a certain task by outputting raw data. So there is a tradeoff between the two. Differential Privacy is not a direct solution to privacy but a mathematical theory to quantify and guide development of private algorithms. Like Asymptotic analysis by itself is not an algorithm by itself but a theory to guide development of algorithms. The reason why it is important to quantify utility is that the adversery interacts with the database in the same manner as the analyst who could benifit from the database.

<br />

While , DP can seem very pessimistic or unnecessary , the issues DP can quantify and guide to solve scale beyond just basic population statistics. Even training Neural Networks. What are the possible harms of neural networks ? Your neural networks could accidentally hint about the possible input data it was trained on. Think about it this way , you are using a NLP service of predicting next word by a MLAAS(Machine Learning As a Service) provider such as Amazon AWS or Microsoft Azure. You could possibly get it to predict a next word that was supposed to confidential or a facial recognition algorithm service where you could show it images of similar people to get an idea of target people to know if they belonged to the training data.

Further , differential privacy does not aim guarantee protection of most individuals data but designed to guarantee every individual's data. The few people with interesting data do form the most interesting insights in analysis and needs to be protected.</p>

This introduces another term used to charecteristic a database called sensitivity. This can be given

<div style="text-align:center">
<img src="https://camo.githubusercontent.com/8f3ce8461e79cf3bb8e7294e36f4fe5e5f8d207e/68747470733a2f2f77696b696d656469612e6f72672f6170692f726573745f76312f6d656469612f6d6174682f72656e6465722f7376672f32646637386532613936666366376166393065336236303366386535373263383337666138303739">
</div>

<p style="text-align:justify">What it intuitively means is that how likely the value of outcome would change if given person was not in the database. Similar to epsilon definition privacy but here we are taking in terms absolute values and not probabilities.Sensitivity is a term that is dependent on the database.

Another guarantee of DP is that it protects individuals data from auxillary information. That is if the data from a given database is combined with data from other database , the data of the individuals still remains private. Individuals could also be identified if their data is combined with auxillary information sources.

Auxillary information sources could refer to external data sources such as another dataset with overlapping attributes date of birth, gender, postal code but with different information which allows the adversery to narrow down choices on the identity of given individual in the database.

One instance of reidentification of individuals in an anonymized dataset is the IMBD and Netflix linkage attack. The details can be found in this paper

Another major property of DP is Composition. Composition not only has a way to quantify how privacy degrades over a single mechanism or query but a series of mechanism or queries. Simplest rule of them is basic composition , which states that if k DP algorithms are run the privacy is now</p>

DP(k<img height="20" width="20" src="https://camo.githubusercontent.com/fbcc26741027732b93efb1ba96c51dd79b6dc404/68747470733a2f2f63646e322e69636f6e66696e6465722e636f6d2f646174612f69636f6e732f677265656b2d6c6174696e2d73796d626f6c732f32342f657073696c6f6e2d3132382e706e67">,k<img height="20" width="20" src="https://camo.githubusercontent.com/ac046ade980b5e0d68df1ad5a1ce38e0e6ed48a6/68747470733a2f2f75706c6f61642e77696b696d656469612e6f72672f77696b6970656469612f636f6d6d6f6e732f7468756d622f392f39662f477265656b5f6c635f64656c74612e7376672f3132303070782d477265656b5f6c635f64656c74612e7376672e706e67">).

<p style="text-align=justify">Composition is an major advantage of DP as it allows to reason about privacy degradation now only with multiple queries but with auxillary information.

I will explain composition in detail in a future article.
</p>
Epsilon delta definition of differential privacy

<img src="https://camo.githubusercontent.com/37263db5c9094e38357fa125e8aec8207f7320b4/68747470733a2f2f692e6962622e636f2f3368786a36316d2f44502d657073696c6f6e2d64656c74612e706e67">

Check out Part II of the tutorials where I will cover some basic DP mechanisms
